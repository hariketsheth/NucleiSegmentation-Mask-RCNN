{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Nucleus Training Data\n",
    "\n",
    "Inspect and visualize data loading and pre-processing code.\n",
    "\n",
    "https://www.kaggle.com/c/data-science-bowl-2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy.core'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy.core'"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpatches\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpatches\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlines\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlines\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\pyplot.py:49\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcycler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cycler\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolorbar\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\colorbar.py:21\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, collections, cm, colors, contour, ticker\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martist\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmartist\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpatches\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpatches\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\collections.py:20\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (_api, _path, artist, cbook, cm, colors \u001b[38;5;28;01mas\u001b[39;00m mcolors, docstring,\n\u001b[0;32m     21\u001b[0m                hatch \u001b[38;5;28;01mas\u001b[39;00m mhatch, lines \u001b[38;5;28;01mas\u001b[39;00m mlines, path \u001b[38;5;28;01mas\u001b[39;00m mpath, transforms)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_enums\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JoinStyle, CapStyle\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# \"color\" is excluded; it is a compound setter, and its docstring differs\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# in LineCollection.\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import math\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import concurrent.futures\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.lines as lines\n",
    "from matplotlib.patches import Polygon\n",
    "import imgaug\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.getcwd()\n",
    "if ROOT_DIR.endswith(\"samples/nucleus\"):\n",
    "    # Go up two levels to the repo root\n",
    "    ROOT_DIR = os.path.dirname(os.path.dirname(ROOT_DIR))\n",
    "    \n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)\n",
    "from mrcnn import utils\n",
    "from mrcnn import visualize\n",
    "from mrcnn.visualize import display_images\n",
    "from mrcnn import model as modellib\n",
    "from mrcnn.model import log\n",
    "\n",
    "import nucleus\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment out to reload imported modules if they change\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset directory\n",
    "DATASET_DIR = os.path.join(ROOT_DIR, \"datasets/nucleus\")\n",
    "\n",
    "# Use configuation from nucleus.py, but override\n",
    "# image resizing so we see the real sizes here\n",
    "class NoResizeConfig(nucleus.NucleusConfig):\n",
    "    IMAGE_RESIZE_MODE = \"none\"\n",
    "    \n",
    "config = NoResizeConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=16):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Adjust the size attribute to control how big to render images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Download the dataset from the competition Website. Unzip it and save it in `mask_rcnn/datasets/nucleus`. If you prefer a different directory then change the `DATASET_DIR` variable above.\n",
    "\n",
    "https://www.kaggle.com/c/data-science-bowl-2018/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = nucleus.NucleusDataset()\n",
    "# The subset is the name of the sub-directory, such as stage1_train,\n",
    "# stage1_test, ...etc. You can also use these special values:\n",
    "#     train: loads stage1_train but excludes validation images\n",
    "#     val: loads validation images from stage1_train. For a list\n",
    "#          of validation images see nucleus.py\n",
    "dataset.load_nucleus(DATASET_DIR, subset=\"train\")\n",
    "\n",
    "# Must call before using the dataset\n",
    "dataset.prepare()\n",
    "\n",
    "print(\"Image Count: {}\".format(len(dataset.image_ids)))\n",
    "print(\"Class Count: {}\".format(dataset.num_classes))\n",
    "for i, info in enumerate(dataset.class_info):\n",
    "    print(\"{:3}. {:50}\".format(i, info['name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset.load_image(image_id)\n",
    "    mask, class_ids = dataset.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset.class_names, limit=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of loading a specific image by its source ID\n",
    "source_id = \"ed5be4b63e9506ad64660dd92a098ffcc0325195298c13c815a73773f1efc279\"\n",
    "\n",
    "# Map source ID to Dataset image_id\n",
    "# Notice the nucleus prefix: it's the name given to the dataset in NucleusDataset\n",
    "image_id = dataset.image_from_source_map[\"nucleus.{}\".format(source_id)]\n",
    "\n",
    "# Load and display\n",
    "image, image_meta, class_ids, bbox, mask = modellib.load_image_gt(\n",
    "        dataset, config, image_id, use_mini_mask=False)\n",
    "log(\"molded_image\", image)\n",
    "log(\"mask\", mask)\n",
    "visualize.display_instances(image, bbox, mask, class_ids, dataset.class_names,\n",
    "                            show_bbox=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Stats\n",
    "\n",
    "Loop through all images in the dataset and collect aggregate stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_stats(image_id):\n",
    "    \"\"\"Returns a dict of stats for one image.\"\"\"\n",
    "    image = dataset.load_image(image_id)\n",
    "    mask, _ = dataset.load_mask(image_id)\n",
    "    bbox = utils.extract_bboxes(mask)\n",
    "    # Sanity check\n",
    "    assert mask.shape[:2] == image.shape[:2]\n",
    "    # Return stats dict\n",
    "    return {\n",
    "        \"id\": image_id,\n",
    "        \"shape\": list(image.shape),\n",
    "        \"bbox\": [[b[2] - b[0], b[3] - b[1]]\n",
    "                 for b in bbox\n",
    "                 # Uncomment to exclude nuclei with 1 pixel width\n",
    "                 # or height (often on edges)\n",
    "                 # if b[2] - b[0] > 1 and b[3] - b[1] > 1\n",
    "                ],\n",
    "        \"color\": np.mean(image, axis=(0, 1)),\n",
    "    }\n",
    "\n",
    "# Loop through the dataset and compute stats over multiple threads\n",
    "# This might take a few minutes\n",
    "t_start = time.time()\n",
    "with concurrent.futures.ThreadPoolExecutor() as e:\n",
    "    stats = list(e.map(image_stats, dataset.image_ids))\n",
    "t_total = time.time() - t_start\n",
    "print(\"Total time: {:.1f} seconds\".format(t_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Size Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image stats\n",
    "image_shape = np.array([s['shape'] for s in stats])\n",
    "image_color = np.array([s['color'] for s in stats])\n",
    "print(\"Image Count: \", image_shape.shape[0])\n",
    "print(\"Height  mean: {:.2f}  median: {:.2f}  min: {:.2f}  max: {:.2f}\".format(\n",
    "    np.mean(image_shape[:, 0]), np.median(image_shape[:, 0]),\n",
    "    np.min(image_shape[:, 0]), np.max(image_shape[:, 0])))\n",
    "print(\"Width   mean: {:.2f}  median: {:.2f}  min: {:.2f}  max: {:.2f}\".format(\n",
    "    np.mean(image_shape[:, 1]), np.median(image_shape[:, 1]),\n",
    "    np.min(image_shape[:, 1]), np.max(image_shape[:, 1])))\n",
    "print(\"Color   mean (RGB): {:.2f} {:.2f} {:.2f}\".format(*np.mean(image_color, axis=0)))\n",
    "\n",
    "# Histograms\n",
    "fig, ax = plt.subplots(1, 3, figsize=(16, 4))\n",
    "ax[0].set_title(\"Height\")\n",
    "_ = ax[0].hist(image_shape[:, 0], bins=20)\n",
    "ax[1].set_title(\"Width\")\n",
    "_ = ax[1].hist(image_shape[:, 1], bins=20)\n",
    "ax[2].set_title(\"Height & Width\")\n",
    "_ = ax[2].hist2d(image_shape[:, 1], image_shape[:, 0], bins=10, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nuclei per Image Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment by image area\n",
    "image_area_bins = [256**2, 600**2, 1300**2]\n",
    "\n",
    "print(\"Nuclei/Image\")\n",
    "fig, ax = plt.subplots(1, len(image_area_bins), figsize=(16, 4))\n",
    "area_threshold = 0\n",
    "for i, image_area in enumerate(image_area_bins):\n",
    "    nuclei_per_image = np.array([len(s['bbox']) \n",
    "                                 for s in stats \n",
    "                                 if area_threshold < (s['shape'][0] * s['shape'][1]) <= image_area])\n",
    "    area_threshold = image_area\n",
    "    if len(nuclei_per_image) == 0:\n",
    "        print(\"Image area <= {:4}**2: None\".format(np.sqrt(image_area)))\n",
    "        continue\n",
    "    print(\"Image area <= {:4.0f}**2:  mean: {:.1f}  median: {:.1f}  min: {:.1f}  max: {:.1f}\".format(\n",
    "        np.sqrt(image_area), nuclei_per_image.mean(), np.median(nuclei_per_image), \n",
    "        nuclei_per_image.min(), nuclei_per_image.max()))\n",
    "    ax[i].set_title(\"Image Area <= {:4}**2\".format(np.sqrt(image_area)))\n",
    "    _ = ax[i].hist(nuclei_per_image, bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nuclei Size Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nuclei size stats\n",
    "fig, ax = plt.subplots(1, len(image_area_bins), figsize=(16, 4))\n",
    "area_threshold = 0\n",
    "for i, image_area in enumerate(image_area_bins):\n",
    "    nucleus_shape = np.array([\n",
    "        b \n",
    "        for s in stats if area_threshold < (s['shape'][0] * s['shape'][1]) <= image_area\n",
    "        for b in s['bbox']])\n",
    "    nucleus_area = nucleus_shape[:, 0] * nucleus_shape[:, 1]\n",
    "    area_threshold = image_area\n",
    "\n",
    "    print(\"\\nImage Area <= {:.0f}**2\".format(np.sqrt(image_area)))\n",
    "    print(\"  Total Nuclei: \", nucleus_shape.shape[0])\n",
    "    print(\"  Nucleus Height. mean: {:.2f}  median: {:.2f}  min: {:.2f}  max: {:.2f}\".format(\n",
    "        np.mean(nucleus_shape[:, 0]), np.median(nucleus_shape[:, 0]),\n",
    "        np.min(nucleus_shape[:, 0]), np.max(nucleus_shape[:, 0])))\n",
    "    print(\"  Nucleus Width.  mean: {:.2f}  median: {:.2f}  min: {:.2f}  max: {:.2f}\".format(\n",
    "        np.mean(nucleus_shape[:, 1]), np.median(nucleus_shape[:, 1]),\n",
    "        np.min(nucleus_shape[:, 1]), np.max(nucleus_shape[:, 1])))\n",
    "    print(\"  Nucleus Area.   mean: {:.2f}  median: {:.2f}  min: {:.2f}  max: {:.2f}\".format(\n",
    "        np.mean(nucleus_area), np.median(nucleus_area),\n",
    "        np.min(nucleus_area), np.max(nucleus_area)))\n",
    "\n",
    "    # Show 2D histogram\n",
    "    _ = ax[i].hist2d(nucleus_shape[:, 1], nucleus_shape[:, 0], bins=20, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nuclei height/width ratio\n",
    "nucleus_aspect_ratio = nucleus_shape[:, 0] / nucleus_shape[:, 1]\n",
    "print(\"Nucleus Aspect Ratio.  mean: {:.2f}  median: {:.2f}  min: {:.2f}  max: {:.2f}\".format(\n",
    "    np.mean(nucleus_aspect_ratio), np.median(nucleus_aspect_ratio),\n",
    "    np.min(nucleus_aspect_ratio), np.max(nucleus_aspect_ratio)))\n",
    "plt.figure(figsize=(15, 5))\n",
    "_ = plt.hist(nucleus_aspect_ratio, bins=100, range=[0, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Augmentation\n",
    "\n",
    "Test out different augmentation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of augmentations\n",
    "# http://imgaug.readthedocs.io/en/latest/source/augmenters.html\n",
    "augmentation = iaa.Sometimes(0.9, [\n",
    "    iaa.Fliplr(0.5),\n",
    "    iaa.Flipud(0.5),\n",
    "    iaa.Multiply((0.8, 1.2)),\n",
    "    iaa.GaussianBlur(sigma=(0.0, 5.0))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load the image multiple times to show augmentations\n",
    "limit = 4\n",
    "ax = get_ax(rows=2, cols=limit//2)\n",
    "for i in range(limit):\n",
    "    image, image_meta, class_ids, bbox, mask = modellib.load_image_gt(\n",
    "        dataset, config, image_id, use_mini_mask=False, augment=False, augmentation=augmentation)\n",
    "    visualize.display_instances(image, bbox, mask, class_ids,\n",
    "                                dataset.class_names, ax=ax[i//2, i % 2],\n",
    "                                show_mask=False, show_bbox=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Crops\n",
    "\n",
    "Microscoy images tend to be large, but nuclei are small. So it's more efficient to train on random crops from large images. This is handled by `config.IMAGE_RESIZE_MODE = \"crop\"`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomCropConfig(nucleus.NucleusConfig):\n",
    "    IMAGE_RESIZE_MODE = \"crop\"\n",
    "    IMAGE_MIN_DIM = 256\n",
    "    IMAGE_MAX_DIM = 256\n",
    "\n",
    "crop_config = RandomCropConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image multiple times to show augmentations\n",
    "limit = 4\n",
    "image_id = np.random.choice(dataset.image_ids, 1)[0]\n",
    "ax = get_ax(rows=2, cols=limit//2)\n",
    "for i in range(limit):\n",
    "    image, image_meta, class_ids, bbox, mask = modellib.load_image_gt(\n",
    "        dataset, crop_config, image_id, use_mini_mask=False)\n",
    "    visualize.display_instances(image, bbox, mask, class_ids,\n",
    "                                dataset.class_names, ax=ax[i//2, i % 2],\n",
    "                                show_mask=False, show_bbox=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Masks\n",
    "\n",
    "Instance binary masks can get large when training with high resolution images. For example, if training with 1024x1024 image then the mask of a single instance requires 1MB of memory (Numpy uses bytes for boolean values). If an image has 100 instances then that's 100MB for the masks alone. \n",
    "\n",
    "To improve training speed, we optimize masks:\n",
    "* We store mask pixels that are inside the object bounding box, rather than a mask of the full image. Most objects are small compared to the image size, so we save space by not storing a lot of zeros around the object.\n",
    "* We resize the mask to a smaller size (e.g. 56x56). For objects that are larger than the selected size we lose a bit of accuracy. But most object annotations are not very accuracy to begin with, so this loss is negligable for most practical purposes. Thie size of the mini_mask can be set in the config class.\n",
    "\n",
    "To visualize the effect of mask resizing, and to verify the code correctness, we visualize some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load random image and mask.\n",
    "image_id = np.random.choice(dataset.image_ids, 1)[0]\n",
    "image = dataset.load_image(image_id)\n",
    "mask, class_ids = dataset.load_mask(image_id)\n",
    "original_shape = image.shape\n",
    "# Resize\n",
    "image, window, scale, padding, _ = utils.resize_image(\n",
    "    image, \n",
    "    min_dim=config.IMAGE_MIN_DIM, \n",
    "    max_dim=config.IMAGE_MAX_DIM,\n",
    "    mode=config.IMAGE_RESIZE_MODE)\n",
    "mask = utils.resize_mask(mask, scale, padding)\n",
    "# Compute Bounding box\n",
    "bbox = utils.extract_bboxes(mask)\n",
    "\n",
    "# Display image and additional stats\n",
    "print(\"image_id: \", image_id, dataset.image_reference(image_id))\n",
    "print(\"Original shape: \", original_shape)\n",
    "log(\"image\", image)\n",
    "log(\"mask\", mask)\n",
    "log(\"class_ids\", class_ids)\n",
    "log(\"bbox\", bbox)\n",
    "# Display image and instances\n",
    "visualize.display_instances(image, bbox, mask, class_ids, dataset.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image_id = np.random.choice(dataset.image_ids, 1)[0]\n",
    "image, image_meta, class_ids, bbox, mask = modellib.load_image_gt(\n",
    "    dataset, config, image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"image\", image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"class_ids\", class_ids)\n",
    "log(\"bbox\", bbox)\n",
    "log(\"mask\", mask)\n",
    "\n",
    "display_images([image]+[mask[:,:,i] for i in range(min(mask.shape[-1], 7))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize.display_instances(image, bbox, mask, class_ids, dataset.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Add augmentation and mask resizing.\n",
    "image, image_meta, class_ids, bbox, mask = modellib.load_image_gt(\n",
    "    dataset, config, image_id, augment=True, use_mini_mask=True)\n",
    "log(\"mask\", mask)\n",
    "display_images([image]+[mask[:,:,i] for i in range(min(mask.shape[-1], 7))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = utils.expand_mask(bbox, mask, image.shape)\n",
    "visualize.display_instances(image, bbox, mask, class_ids, dataset.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anchors\n",
    "\n",
    "For an FPN network, the anchors must be ordered in a way that makes it easy to match anchors to the output of the convolution layers that predict anchor scores and shifts. \n",
    "* Sort by pyramid level first. All anchors of the first level, then all of the second and so on. This makes it easier to separate anchors by level.\n",
    "* Within each level, sort anchors by feature map processing sequence. Typically, a convolution layer processes a feature map starting from top-left and moving right row by row. \n",
    "* For each feature map cell, pick any sorting order for the anchors of different ratios. Here we match the order of ratios passed to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize anchors of one cell at the center of the feature map\n",
    "\n",
    "# Load and display random image\n",
    "image_id = np.random.choice(dataset.image_ids, 1)[0]\n",
    "image, image_meta, _, _, _ = modellib.load_image_gt(dataset, crop_config, image_id)\n",
    "\n",
    "# Generate Anchors\n",
    "backbone_shapes = modellib.compute_backbone_shapes(config, image.shape)\n",
    "anchors = utils.generate_pyramid_anchors(config.RPN_ANCHOR_SCALES, \n",
    "                                          config.RPN_ANCHOR_RATIOS,\n",
    "                                          backbone_shapes,\n",
    "                                          config.BACKBONE_STRIDES, \n",
    "                                          config.RPN_ANCHOR_STRIDE)\n",
    "\n",
    "# Print summary of anchors\n",
    "num_levels = len(backbone_shapes)\n",
    "anchors_per_cell = len(config.RPN_ANCHOR_RATIOS)\n",
    "print(\"Count: \", anchors.shape[0])\n",
    "print(\"Scales: \", config.RPN_ANCHOR_SCALES)\n",
    "print(\"ratios: \", config.RPN_ANCHOR_RATIOS)\n",
    "print(\"Anchors per Cell: \", anchors_per_cell)\n",
    "print(\"Levels: \", num_levels)\n",
    "anchors_per_level = []\n",
    "for l in range(num_levels):\n",
    "    num_cells = backbone_shapes[l][0] * backbone_shapes[l][1]\n",
    "    anchors_per_level.append(anchors_per_cell * num_cells // config.RPN_ANCHOR_STRIDE**2)\n",
    "    print(\"Anchors in Level {}: {}\".format(l, anchors_per_level[l]))\n",
    "\n",
    "# Display\n",
    "fig, ax = plt.subplots(1, figsize=(10, 10))\n",
    "ax.imshow(image)\n",
    "levels = len(backbone_shapes)\n",
    "\n",
    "for level in range(levels):\n",
    "    colors = visualize.random_colors(levels)\n",
    "    # Compute the index of the anchors at the center of the image\n",
    "    level_start = sum(anchors_per_level[:level]) # sum of anchors of previous levels\n",
    "    level_anchors = anchors[level_start:level_start+anchors_per_level[level]]\n",
    "    print(\"Level {}. Anchors: {:6}  Feature map Shape: {}\".format(level, level_anchors.shape[0], \n",
    "                                                                backbone_shapes[level]))\n",
    "    center_cell = backbone_shapes[level] // 2\n",
    "    center_cell_index = (center_cell[0] * backbone_shapes[level][1] + center_cell[1])\n",
    "    level_center = center_cell_index * anchors_per_cell \n",
    "    center_anchor = anchors_per_cell * (\n",
    "        (center_cell[0] * backbone_shapes[level][1] / config.RPN_ANCHOR_STRIDE**2) \\\n",
    "        + center_cell[1] / config.RPN_ANCHOR_STRIDE)\n",
    "    level_center = int(center_anchor)\n",
    "\n",
    "    # Draw anchors. Brightness show the order in the array, dark to bright.\n",
    "    for i, rect in enumerate(level_anchors[level_center:level_center+anchors_per_cell]):\n",
    "        y1, x1, y2, x2 = rect\n",
    "        p = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, facecolor='none',\n",
    "                              edgecolor=(i+1)*np.array(colors[level]) / anchors_per_cell)\n",
    "        ax.add_patch(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create data generator\n",
    "random_rois = 2000\n",
    "g = modellib.data_generator(\n",
    "    dataset, crop_config, shuffle=True, random_rois=random_rois, \n",
    "    batch_size=4,\n",
    "    detection_targets=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Uncomment to run the generator through a lot of images\n",
    "# to catch rare errors\n",
    "# for i in range(1000):\n",
    "#     print(i)\n",
    "#     _, _ = next(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get Next Image\n",
    "if random_rois:\n",
    "    [normalized_images, image_meta, rpn_match, rpn_bbox, gt_class_ids, gt_boxes, gt_masks, rpn_rois, rois], \\\n",
    "    [mrcnn_class_ids, mrcnn_bbox, mrcnn_mask] = next(g)\n",
    "    \n",
    "    log(\"rois\", rois)\n",
    "    log(\"mrcnn_class_ids\", mrcnn_class_ids)\n",
    "    log(\"mrcnn_bbox\", mrcnn_bbox)\n",
    "    log(\"mrcnn_mask\", mrcnn_mask)\n",
    "else:\n",
    "    [normalized_images, image_meta, rpn_match, rpn_bbox, gt_boxes, gt_masks], _ = next(g)\n",
    "    \n",
    "log(\"gt_class_ids\", gt_class_ids)\n",
    "log(\"gt_boxes\", gt_boxes)\n",
    "log(\"gt_masks\", gt_masks)\n",
    "log(\"rpn_match\", rpn_match, )\n",
    "log(\"rpn_bbox\", rpn_bbox)\n",
    "image_id = modellib.parse_image_meta(image_meta)[\"image_id\"][0]\n",
    "print(\"image_id: \", image_id, dataset.image_reference(image_id))\n",
    "\n",
    "# Remove the last dim in mrcnn_class_ids. It's only added\n",
    "# to satisfy Keras restriction on target shape.\n",
    "mrcnn_class_ids = mrcnn_class_ids[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "b = 0\n",
    "\n",
    "# Restore original image (reverse normalization)\n",
    "sample_image = modellib.unmold_image(normalized_images[b], config)\n",
    "\n",
    "# Compute anchor shifts.\n",
    "indices = np.where(rpn_match[b] == 1)[0]\n",
    "refined_anchors = utils.apply_box_deltas(anchors[indices], rpn_bbox[b, :len(indices)] * config.RPN_BBOX_STD_DEV)\n",
    "log(\"anchors\", anchors)\n",
    "log(\"refined_anchors\", refined_anchors)\n",
    "\n",
    "# Get list of positive anchors\n",
    "positive_anchor_ids = np.where(rpn_match[b] == 1)[0]\n",
    "print(\"Positive anchors: {}\".format(len(positive_anchor_ids)))\n",
    "negative_anchor_ids = np.where(rpn_match[b] == -1)[0]\n",
    "print(\"Negative anchors: {}\".format(len(negative_anchor_ids)))\n",
    "neutral_anchor_ids = np.where(rpn_match[b] == 0)[0]\n",
    "print(\"Neutral anchors: {}\".format(len(neutral_anchor_ids)))\n",
    "\n",
    "# ROI breakdown by class\n",
    "for c, n in zip(dataset.class_names, np.bincount(mrcnn_class_ids[b].flatten())):\n",
    "    if n:\n",
    "        print(\"{:23}: {}\".format(c[:20], n))\n",
    "\n",
    "# Show positive anchors\n",
    "fig, ax = plt.subplots(1, figsize=(16, 16))\n",
    "visualize.draw_boxes(sample_image, boxes=anchors[positive_anchor_ids], \n",
    "                     refined_boxes=refined_anchors, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show negative anchors\n",
    "visualize.draw_boxes(sample_image, boxes=anchors[negative_anchor_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show neutral anchors. They don't contribute to training.\n",
    "visualize.draw_boxes(sample_image, boxes=anchors[np.random.choice(neutral_anchor_ids, 100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROIs\n",
    "\n",
    "Typically, the RPN network generates region proposals (a.k.a. Regions of Interest, or ROIs). The data generator has the ability to generate proposals as well for illustration and testing purposes. These are controlled by the `random_rois` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if random_rois:\n",
    "    # Class aware bboxes\n",
    "    bbox_specific = mrcnn_bbox[b, np.arange(mrcnn_bbox.shape[1]), mrcnn_class_ids[b], :]\n",
    "\n",
    "    # Refined ROIs\n",
    "    refined_rois = utils.apply_box_deltas(rois[b].astype(np.float32), bbox_specific[:,:4] * config.BBOX_STD_DEV)\n",
    "\n",
    "    # Class aware masks\n",
    "    mask_specific = mrcnn_mask[b, np.arange(mrcnn_mask.shape[1]), :, :, mrcnn_class_ids[b]]\n",
    "\n",
    "    visualize.draw_rois(sample_image, rois[b], refined_rois, mask_specific, mrcnn_class_ids[b], dataset.class_names)\n",
    "    \n",
    "    # Any repeated ROIs?\n",
    "    rows = np.ascontiguousarray(rois[b]).view(np.dtype((np.void, rois.dtype.itemsize * rois.shape[-1])))\n",
    "    _, idx = np.unique(rows, return_index=True)\n",
    "    print(\"Unique ROIs: {} out of {}\".format(len(idx), rois.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if random_rois:\n",
    "    # Dispalay ROIs and corresponding masks and bounding boxes\n",
    "    ids = random.sample(range(rois.shape[1]), 8)\n",
    "\n",
    "    images = []\n",
    "    titles = []\n",
    "    for i in ids:\n",
    "        image = visualize.draw_box(sample_image.copy(), rois[b,i,:4].astype(np.int32), [255, 0, 0])\n",
    "        image = visualize.draw_box(image, refined_rois[i].astype(np.int64), [0, 255, 0])\n",
    "        images.append(image)\n",
    "        titles.append(\"ROI {}\".format(i))\n",
    "        images.append(mask_specific[i] * 255)\n",
    "        titles.append(dataset.class_names[mrcnn_class_ids[b,i]][:20])\n",
    "\n",
    "    display_images(images, titles, cols=4, cmap=\"Blues\", interpolation=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check ratio of positive ROIs in a set of images.\n",
    "if random_rois:\n",
    "    limit = 10\n",
    "    temp_g = modellib.data_generator(\n",
    "        dataset, crop_config, shuffle=True, random_rois=10000, \n",
    "        batch_size=1, detection_targets=True)\n",
    "    total = 0\n",
    "    for i in range(limit):\n",
    "        _, [ids, _, _] = next(temp_g)\n",
    "        positive_rois = np.sum(ids[0] > 0)\n",
    "        total += positive_rois\n",
    "        print(\"{:5} {:5.2f}\".format(positive_rois, positive_rois/ids.shape[1]))\n",
    "    print(\"Average percent: {:.2f}\".format(total/(limit*ids.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
